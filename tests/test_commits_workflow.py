"""Test the whole commits workflow script."""

import pytest
import json
import pandas as pd

from githubanalysis.processing.get_all_branches_commits import AllBranchesCommitsGetter
import githubanalysis.processing.run_commits_workflow as runner
from utilities.check_gh_reponse import (
    RateLimitError,
    RepoNotFoundError,
    UnexpectedAPIError,
)
import utilities.get_default_logger as loggit

# def test_function():
#     # Arrange:

#     # Act:

#     # Assert:

rawjson = "tests/testdata/raw-commits__all-branches-commits_FlicAnderson-peramagroon_2024-10-17.json"
deduplicatedjson = "tests/testdata/deduplicated-commits__all-branches-commits_FlicAnderson-peramagroon_2024-10-17_deduplicated.json"
processed_csv = "tests/testdata/processed-csv__processed-commits_FlicAnderson-peramagroon_2024-10-17.csv"
commit_details = "tests/testdata/commit-details__commits_changes_FlicAnderson-peramagroon_2024-10-17.csv"
workflow_outputs = "tests/testdata/workflow-outputs__commits_cats_stats_FlicAnderson-peramagroon_2024-10-17.csv"


logger = loggit.get_default_logger(
    console=True,
    set_level_to="DEBUG",
    log_name="logs/testing_logs.txt",
    in_notebook=False,
)


def read_in_csv_file(filename: str) -> pd.DataFrame:
    return pd.read_csv(filename, index_col=0, header=0)


def get_number_commits_in_dict(d: dict[str, list[str]]) -> int:
    """
    This allows calculation of the number of items within a dictionary
    of lists of strings such as the datastructure generated by
    githubanalysis.processing.get_all_branches_commits() within commits
    workflow.
    """
    return sum([len(d[x]) for x in d if isinstance(d[x], list)])


def test_outputs_read_in_match():
    """
    Check output files generated by the different stages of commits
    workflow are same length.

    WARNING: This may NOT always be the case for real-world data, as
    empty commits are dropped during the workflow intentionally.
    TODO: Ideally a future test would check whether:
    len(previous step output - NA rows) == len(subsequent step output)
    """
    # Arrange:

    processed_csv_df = read_in_csv_file(processed_csv)
    commit_details_df = read_in_csv_file(commit_details)
    workflow_outputs_df = read_in_csv_file(workflow_outputs)
    assert (
        len(processed_csv_df) == len(commit_details_df) == len(workflow_outputs_df)
    ), "WARNING: The test output files are not of same length. This could be due to intentionally dropped NAs. "

    allbranchescommitsgetter = AllBranchesCommitsGetter(
        repo_name="FlicAnderson/peramagroon",
        in_notebook=False,
        config_path="githubanalysis/config.cfg",
        logger=None,
    )

    # Act:
    output_get_all_branches_commits = allbranchescommitsgetter.get_all_branches_commits(
        repo_name="FlicAnderson/peramagroon"
    )

    # Assert:
    assert (  # output matches read-in number of processed commits csv rows
        (get_number_commits_in_dict(output_get_all_branches_commits))
        == len(processed_csv_df.index)
    ), f"Length of values in dict generated by get_all_branches_commits() ({get_number_commits_in_dict(output_get_all_branches_commits)}) does not match number of rows read in from processed-csv .csv file ({len(processed_csv_df.index)})"

    # Assert:
    assert (  # output matches read-in number of workflow outputs csv rows
        (get_number_commits_in_dict(output_get_all_branches_commits))
        == len(workflow_outputs_df.index)
    ), f"Length of values in dict generated by get_all_branches_commits() ({get_number_commits_in_dict(output_get_all_branches_commits)}) does not match number of rows read in from workflow outputs .csv file ({len(processed_csv_df.index)}. This COULD be due to intentionally dropped NAs - check carefully to ensure this is as expected.)"


@pytest.mark.xfail(reason="Fails remotely: relies on GH config file")
def test_commits_workflow_raises_404():
    """
    This should confirm that the workflow reacts appropriately
    (RepoNotFoundError at 404) when given an inaccessible repo_name.
    This runs GH API code.
    """
    # Arrange:
    repo_404 = "FlicAnderson/NonexistentRepo"

    runcommits = runner.RunCommits(
        repo_name=repo_404,
        in_notebook=False,
        config_path="githubanalysis/config.cfg",
        write_read_location="data/",
        logger=logger,
    )

    # Assert:
    with pytest.raises(RepoNotFoundError):
        runcommits.do_it_all()


@pytest.mark.xfail(reason="Fails remotely: relies on GH config file")
def test_run_commits_workflow_succeeds():
    """
    This should confirm that the workflow reacts appropriately
    (RepoNotFoundError at 404) when given an inaccessible repo_name.
    This runs GH API code.
    """
    # Arrange:
    repo = "FlicAnderson/peramagroon"

    runcommits = runner.RunCommits(
        repo_name=repo,
        in_notebook=False,
        config_path="githubanalysis/config.cfg",
        write_read_location="data/",
        logger=logger,
    )

    # Assert:
    assert len(runcommits.do_it_all()) == len(
        read_in_csv_file(workflow_outputs)
    ), "Workflow outputs lengths don't match anymore."
