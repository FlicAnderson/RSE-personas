""" Function to get assignment types for all dev in given GH repo, joining issues and contributor datasets. """

import sys
import os
import numpy as np
import pandas as pd
import datetime
from datetime import datetime
from ast import literal_eval
import logging

import utilities.get_default_logger as loggit
import githubanalysis.processing.get_all_pages_issues
from githubanalysis.processing.get_all_pages_issues import IssueGetter
from githubanalysis.processing.get_repo_contributors import DevsGetter

class DevsAssigner:

    # if not given a better option, use my default settings for logging
    logger: logging.Logger
    def __init__(self, logger: logging.Logger = None) -> None:
        if logger is None:
            self.logger = loggit.get_default_logger(console=False, set_level_to='INFO', log_name='logs/get_all_devs_assignment_logs.txt')
        else:
            self.logger = logger

    def get_all_devs_assignment(self, repo_name, issues_filename=None, contributors_filename=None, read_in_location='data/', out_filename='devs-assignments', write_out_location='data/', config_path='githubanalysis/config.cfg'):
    
        """
        Get all devs in repo repo_name.
        Ensure issues/PRs dataset exists, if not, create it.
        Assign dev_name to assignment categories to each dev based on issue tickets and PRs;  
        Return assignments dataset devs_assignments_df. 
        
        :param repo_name: GH repository name  
        :type: string
        :param issues_filename: filename (prefix only, no '.csv' - e.g. 'all-issues_JeschkeLab-DeerLab_2024-02-27') of issues dataset generated by get_all_pages_issues(). Default: None  
        :type: string
        :param contributors_filename: filename (prefix only, no '.csv' - e.g. 'contributors_JeschkeLab-DeerLab_2024-02-27') of contributor dataset generated by get_repo_contributors(). Default: None
        :type: string
        :returns: `devs_assignments_df` with key categories 'dev_name' and 'assignment', and assignment counts from issues dataset, as well as 'contributions' counts from contributor dataset.
        :type: pd.df

        """

                # get repo_name from commandline if given (accept commandline input) 
        if len(sys.argv) >= 2:
            repo_name = sys.argv[1]  # use second argv (user-provided by commandline)
            self.logger.debug(f"Using commandline argument {repo_name} as repo name to get dev issue/PR assignments for. Entered as: {sys.argv[1]}. ")
            if not isinstance(repo_name, str):
                raise TypeError("Ensure argument is repository name in string format (e.g. 'repo-owner/repo-name')")
            
            if len(sys.argv) >= 3:
                issues_filename = sys.argv[2]  # use third argv (user-provided by commandline)
                self.logger.debug(f"Using commandline argument {issues_filename} as issues filename to get dev issue/PR assignments for. Entered as: {sys.argv[2]}. ")
                if not isinstance(issues_filename, str):
                    raise TypeError("Ensure argument is issues dataset in string format (e.g. 'all-issues_JeschkeLab-DeerLab_2024-02-27')")
            
            if len(sys.argv) >= 4:
                    contributors_filename = sys.argv[3] # use fourth argv as location of issues data if given  
                    self.logger.debug(f"Using commandline argument {contributors_filename} as contributors filename to get dev issue/PR assignments for. Entered as: {sys.argv[3]}. ")            
                    if not isinstance(contributors_filename, str):
                        raise TypeError("Ensure argument is contributors dataset in string format (e.g. 'all-issues_JeschkeLab-DeerLab_2024-02-27')")           
        else: 
            try:
                repo_name = repo_name
                issues_filename = issues_filename
                contributors_filename = contributors_filename
                self.logger.debug(f"Repo name is {repo_name}. Getting issues.")
                self.logger.debug(f"Issues dataset is {issues_filename}.")
                self.logger.debug(f"Contributors dataset is {contributors_filename}.")
            except:
                raise IndexError("Please enter a repo_name (NECCESSARY), issues_filename (optional) and contributors_filename (optional) all as 'quoted strings' separated by a space...")

        #print(f"{repo_name}; {issues_filename}; {contributors_filename}.")

        # write-out file setup     
        # get date for generating extra filename info
        current_date_info = datetime.now().strftime("%Y-%m-%d") # run this at start of script not in loop to avoid midnight/long-run issues
        sanitised_repo_name = repo_name.replace("/", "-")
        write_out = f'{write_out_location}{out_filename}_{sanitised_repo_name}'
        write_out_extra_info = f"{write_out}_{current_date_info}.csv"  

        #check issues/PRs data file exists already; if not, get that.  
        try:
            issues_filepath = f"{read_in_location}{issues_filename}.csv"
            #print(issues_filepath)
            try:
                if os.path.isfile(issues_filename):
                    self.logger.debug("issues filepath seems to exist ok.")
                else:
                    self.logger.debug(f"issues_filepath {issues_filepath} exists = {os.path.isfile(issues_filepath)}.")
            except:
                FileExistsError(f"File {issues_filepath} does not seem to exist...?")
        except Exception as e_read_issues:
            self.logger.error(f"Error in reading in issues dataset ({issues_filepath}) for repo {repo_name}: {e_read_issues}")
          
        try:
            contributors_filepath = f"{read_in_location}{contributors_filename}.csv"
            # read in contributors data and rename 'login' field as 'dev_name' as later joining key  
            contributors_df = pd.read_csv(contributors_filepath, header=0)
            contributors_df['dev_name'] = contributors_df['login']
        except Exception as e_read_contributors:
            self.logger.error(f"Error in reading in contributors dataset ({contributors_filepath}) for repo {repo_name}: {e_read_contributors}")

        issues_df = pd.read_csv(issues_filepath, header=0)
        exploded_devs = pd.DataFrame()
        exploded_devs = issues_df

        # handle multi-dev assignments by pd.DataFrame.explode() to lengthen dataset 
        exploded_devs['assigned_devs'] = issues_df['assigned_devs'].apply(literal_eval)
        exploded_devs = exploded_devs.explode(column='assigned_devs')
        exploded_devs['assigned_devs'] = exploded_devs['assigned_devs'].fillna('unassigned') 

        # group by dev
        grouped = exploded_devs.groupby('assigned_devs')

        all_devs = []
        dev_dict = {}

        for name, group in grouped:
            total_recs = group['id'].count()
            total_PRs = group['is_PR'].sum()
            dev_dict.update({'dev_name': name})
            dev_dict.update({'total': total_recs})
            dev_dict.update({'PRs': total_PRs})
            dev_dict.update({'issues': total_recs - total_PRs})
            
            all_devs.append(dev_dict)
            dev_dict = {}

        # create df from the dict
        all_devs_df = pd.DataFrame.from_dict(all_devs)
        
        # get all devs data together:
        # do outer join on both issues and contributors datasets on dev_name column (full outer join so we don't lose any devs not in other df)    
        devs_assignments_df = pd.merge(all_devs_df, contributors_df, how='outer', on=['dev_name'])

        # drop extra columns brought in from contributors dataset  
        devs_assignments_df = devs_assignments_df[['dev_name', 'total', 'PRs', 'issues', 'login', 'type', 'contributions', 'email', 'name']]

        # fill columns from all_devs_df with 0s for newly added devs w/o assignments info from other table    
        devs_assignments_df['total'] = devs_assignments_df['total'].fillna(value=0)
        devs_assignments_df['PRs'] = devs_assignments_df['PRs'].fillna(value=0)
        devs_assignments_df['issues'] = devs_assignments_df['issues'].fillna(value=0)

        # assign category based on thresholds of >1 of both, either or none. 
        conditions = [
            (devs_assignments_df['PRs'] >= 1) & (devs_assignments_df['issues'] >= 1),
            (devs_assignments_df['PRs'] == 0) & (devs_assignments_df['issues'] >=1),
            (devs_assignments_df['PRs'] >=1) & (devs_assignments_df['issues'] == 0),
            (devs_assignments_df['PRs'] == 0) & (devs_assignments_df['issues'] == 0)
        ]
        choices = ['issues_and_PRs','issues_only', 'PRs_only', 'neither']
        devs_assignments_df['assignment'] = np.select(conditions, choices, default='error')

        # remove non-named-dev entries from joined dataset  
        devs_assignments_df = devs_assignments_df.drop(devs_assignments_df[devs_assignments_df['dev_name'] == 'unassigned'].index)
        devs_assignments_df = devs_assignments_df.drop(devs_assignments_df[devs_assignments_df['dev_name'].isna()].index)

        # make assignment a category datatype for easier future work (optional in script probably)
        devs_assignments_df['assignment'] = devs_assignments_df['assignment'].astype('category')
        # set the other categories so whole set is there if not present from data  
        devs_assignments_df['assignment'].cat.set_categories(['issues_and_PRs','issues_only', 'PRs_only', 'neither'])

        # write out dataset: 
        devs_assignments_df.to_csv(write_out_extra_info, mode='w', index=True, header=True)

        #return 
        return devs_assignments_df

        
# this bit
if __name__ == "__main__":
    """
    gets issue/PR assignment for all devs in specific GH repo. 
    """

    logger = loggit.get_default_logger(console=True, set_level_to='DEBUG', log_name='logs/get_all_devs_assignment_logs.txt')

    devs_assigner = DevsAssigner(logger)

    if len(sys.argv) >= 2:
        repo_name = sys.argv[1]  # use second argv (user-provided by commandline)
        logger.debug(f"Using commandline argument {repo_name} as repo name to get dev issue/PR assignments for. Entered as: {sys.argv[1]}. ")
        if not isinstance(repo_name, str):
            raise TypeError("Ensure argument is repository name in string format (e.g. 'repo-owner/repo-name')")
        if len(sys.argv) >= 3:
            issues_filename = sys.argv[2]  # use third argv (user-provided by commandline)
            logger.debug(f"Using commandline argument {issues_filename} as issues filename to get dev issue/PR assignments for. Entered as: {sys.argv[2]}. ")
            if not isinstance(issues_filename, str):
                raise TypeError("Ensure argument is issues dataset in string format (e.g. 'all-issues_JeschkeLab-DeerLab_2024-02-27')")
        if len(sys.argv) >= 4:
            contributors_filename = sys.argv[3] # use fourth argv as location of issues data if given  
            logger.debug(f"Using commandline argument {contributors_filename} as contributors filename to get dev issue/PR assignments for. Entered as: {sys.argv[3]}. ")            
            if not isinstance(contributors_filename, str):
                raise TypeError("Ensure argument is contributors dataset in string format (e.g. 'all-issues_JeschkeLab-DeerLab_2024-02-27')")           

    devs_assignments_df = pd.DataFrame()

    try:
        devs_assignments_df = devs_assigner.get_all_devs_assignment(repo_name=repo_name, issues_filename=issues_filename, contributors_filename=contributors_filename, read_in_location='data/', out_filename='devs-assignments', write_out_location='data/', config_path='githubanalysis/config.cfg')
        if len(devs_assignments_df) != 0:
            logger.info(f"Number of devs given assignment category for repo {repo_name} is {len(devs_assignments_df.index)}.")
            logger.info(f"Dev assignments: {devs_assignments_df['assignment'].value_counts()}.")
        else:
            logger.warning("Getting all dev assignments did not work, length of returned records is zero.")
    except Exception as e:
        logger.error(f"Exception while running get_all_devs_assignment() on repo {repo_name}: {e}")