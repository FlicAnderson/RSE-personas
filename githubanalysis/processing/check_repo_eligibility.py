"""Checks whether given GitHub repository is eligible for analysis."""

from pathlib import Path
import argparse
import datetime
import pandas as pd
import yaml
import logging

import utilities.get_default_logger as loggit
from utilities.repo_names_write_out import RepoNamesListCreator


class GenerateStudySample:
    logger: logging.Logger
    in_notebook: bool
    current_date_info: str
    write_location: Path
    read_location: Path

    def __init__(
        self,
        in_notebook: bool,
        logger: None | logging.Logger = None,
    ) -> None:
        if logger is None:
            self.logger = loggit.get_default_logger(
                console=False,
                set_level_to="DEBUG",
                log_name="logs/check_repo_eligibility.txt",
                in_notebook=in_notebook,
            )
        else:
            self.logger = logger

        self.in_notebook = in_notebook
        # write-out file setup
        self.current_date_info = datetime.datetime.now().strftime(
            "%Y-%m-%d"
        )  # at start of script to avoid midnight/long-run issues
        self.read_location = Path("data/" if not in_notebook else "../../data/")
        self.write_location = Path("data/" if not in_notebook else "../../data/")

    def write_out_eligible_repos(
        self,
        study_sample_df: pd.DataFrame,
        read_location: str | Path = "data/",
        write_location: str | Path = "data/",
    ) -> pd.DataFrame | None:
        if study_sample_df is not None:
            n_repos_sample = int(study_sample_df.groupby("repo_name").ngroups)

        df_filestr = f"study-sample-summary-data_x{n_repos_sample}-repos_{self.current_date_info}.csv"
        df_writeout_path = Path(write_location, df_filestr)

        eligible_names_filestr = "study-sample-repo-names"

        study_sample_df.to_csv(path_or_buf=df_writeout_path, header=True, index=False)

        self.logger.info(f"Merged dataset file written out to {df_writeout_path}")

        repo_names_list_creator = RepoNamesListCreator(
            in_notebook=self.in_notebook,
            logger=self.logger,
        )

        eligible_repo_names = list(study_sample_df["repo_name"])
        # self.logger.info(f"eligible names type: {type(eligible_repo_names)}")
        # self.logger.info(f"eligible names type: {len(eligible_repo_names)}")

        repo_names_list_creator.repo_names_write_out(
            namelist=eligible_repo_names,
            repo_name_filename=eligible_names_filestr,
            write_location=self.write_location,
        )

        self.logger.info(f"Merged dataset file written out to {df_writeout_path}")

        logger.info(
            f"Eligible repos summary data csv file written out to {df_writeout_path}"
        )
        # logger.info(
        #     f"Eligible repos names list file written out to something like{names_writeout_path}"
        # )

    def check_repo_eligibility(
        self,
        population_set: pd.DataFrame,
        read_location: str | Path = "data/",
        write_location: str | Path = "data/",
    ) -> pd.DataFrame | None:
        """
        Reads in summarised_repo_stats logfile generated by read_summary_stats_log.py
        and checks whether each GitHub repository meets eligibility criteria
        for analysis in coding-smart study.
        Returns csv file with additional colummn(s) of eligibility bool (?and additional information?)

        :param in_filename: name of CSV file to read in, excluding file extension. Default = "summarised_repo_stats".
        :type: str
        :param read_in_location: path of Github URLs file as string. Default = "data/"
        :type: str
        :returns: `summarised_repo_stats_eligible_df`
        :type: pd.DataFrame df equivalent to summarised_repo_stats with additional column(s)

        Examples:
        ----------
        >>> python githubanalysis/processing/check_repo_eligibility.py -f data/summarised_repo_stats_2025-04-15.csv
        """
        start_time = datetime.datetime.now()

        if population_set is None or population_set.empty:
            raise RuntimeError(
                "Population Set given is None or dataframe is empty; check input"
            )

        n_population = len(population_set.index)  # total population of RS repos

        population_set.loc[:, "devs_above_mean"] = population_set["devs"].apply(
            lambda x: x > population_set.devs.mean()
        )  # > 10devs
        population_set.loc[:, "devs_within_1SD"] = population_set["devs"].apply(
            lambda x: x < population_set.devs.std()
        )  # < 153.67 devs
        population_set.loc[:, "devs_within_2SD"] = population_set["devs"].apply(
            lambda x: x < (population_set.devs.std() * 2)
        )  # < 307.34 devs

        # has open license allowing me to work w/ it
        yamlpath = Path(read_location, "permissive_licenses.yaml")
        self.logger.info(f"License yaml file at: {yamlpath}")

        try:
            with open(file=yamlpath, mode="r") as file:
                yaml_licenses = yaml.safe_load(file)
            # self.logger.debug(yaml_licenses)

        except Exception as e:
            self.logger.error(f"licenses ain't right: {e}")
            raise RuntimeError("yaml file isn't loading somehow")

        self.logger.info(f"{yaml_licenses['accept_licenses']}")

        population_set.loc[:, "licensed_to_use"] = population_set["repo_license"].apply(
            lambda x: x in yaml_licenses["accept_licenses"]
        )

        # run exclusion
        try:
            eligible_repos = population_set.query(
                "licensed_to_use == True & repo_is_fork == False & devs_above_mean == True & devs_within_2SD == True & repo_age_days >= 1000"
            )
            print(
                f"{len(eligible_repos)} to be kept: has permissive license, not fork (API value), number of devs (total contributors from GH repos API) is above population mean and less than than 2SD of population devs and repo more than 1000 days old"
            )
            excl_dataset_repos = n_population - len(eligible_repos)
            print(
                f"{excl_dataset_repos} to be excluded: no license, is fork, less than 10devs, more than 2SD of devs, less than 1000 days old"
            )
            print(
                f"This means {n_population - excl_dataset_repos} ({round((n_population - excl_dataset_repos) / n_population*100, 2)}%) REMAIN of total {n_population} population RS repos"
            )
            print(
                f"Returning {len(eligible_repos)} eligible repositories for sample set."
            )

            # write out df of eligible repos summary data and as repo names list text file:
            self.write_out_eligible_repos(
                study_sample_df=eligible_repos,
                read_location=self.read_location,
                write_location=self.write_location,
            )

            end_time = datetime.datetime.now()

            logger.info(
                f"Run time for {population_set.groupby('repo_name').ngroups} repos: {end_time - start_time}."
            )

            return eligible_repos

        except Exception as e:
            self.logger.error(f"Failed to run exclusion query with error {e}.")

    def read_population_stats(
        self,
        summary_repo_stats_file="summarised_repo_stats.csv",
        read_location: str | Path = "data/",
        write_location: str | Path = "data/",
    ):
        population_set = pd.read_csv(
            filepath_or_buffer=summary_repo_stats_file, header=0
        )
        self.logger.info(f"population_set dataframe has {len(population_set)} rows.")

        if population_set is None or population_set.empty:
            raise RuntimeError(
                "Population Set given is None or dataframe is empty; check input"
            )

        population_set.drop_duplicates(keep="last", inplace=True)

        self.logger.debug(
            f"population_set df has {population_set.groupby('repo_name').ngroups} unique repo_names AFTER DEDUPLICATION (all columns)."
        )

        population_set.drop_duplicates(subset=["repo_name"], keep="last", inplace=True)

        self.logger.debug(
            f"population_set df has {population_set.groupby('repo_name').ngroups} unique repo_names AFTER DEDUPLICATION (repo_name only)."
        )

        self.logger.info(
            f"Population dataset contains {population_set.groupby('repo_name').ngroups} repositories"
        )

        return population_set


parser = argparse.ArgumentParser()
parser.add_argument(
    "-f",
    "--summary-repo-data-file",
    metavar="SUMMARY_REPO_DATA_FILE",
    help="Path to .csv file containing commits data (line per repo-individual), eg 'commits-data-per-dev_x5828-repos_2025-02-15.csv'.",
    type=str,
)


if __name__ == "__main__":
    args = parser.parse_args()
    summary_repo_file: str = args.summary_repo_data_file

    start_time = datetime.datetime.now()

    logger = loggit.get_default_logger(
        console=True,
        set_level_to="DEBUG",
        log_name="logs/check_repo_eligibility.txt",
        in_notebook=False,
    )

    logger.info("Running eligibility check process.")

    generate_study_sample = GenerateStudySample(in_notebook=False, logger=logger)

    population_set = generate_study_sample.read_population_stats(
        summary_repo_stats_file=summary_repo_file,
        read_location="data/",
        write_location="data/",
    )
    try:
        if population_set is not None:
            # apply the eligibility criteria function to the population dataset
            study_sample = generate_study_sample.check_repo_eligibility(
                population_set=population_set,
            )
            if study_sample is not None:
                logger.info(
                    f"There are {study_sample.groupby('repo_name').ngroups} eligible repositories from a total of {population_set.groupby('repo_name').ngroups} population repositories."
                )
                logger.info(f"Study sample df has length: {len(study_sample)}.")

                sample_pc = round(
                    (  # generate rounded (2dp) percentage: sample of population
                        (
                            study_sample.groupby("repo_name").ngroups
                            / population_set.groupby("repo_name").ngroups
                        )
                        * 100
                    ),
                    2,
                )
                logger.info(
                    f"Study Sample ({study_sample.groupby('repo_name').ngroups} repos) represents {sample_pc}% of Population Set ({population_set.groupby('repo_name').ngroups} repos)."
                )

            end_time = datetime.datetime.now()

            logger.info(
                f"Run time for generating study sample set of {study_sample.groupby('repo_name').ngroups} repos: {end_time - start_time}."
            )

    except Exception as e:
        logger.error(
            f"Error in generating the study sample from the population set repo summary data; {e}; error type: {type(e)}"
        )
        raise RuntimeError("failed to generate the study sample.")
