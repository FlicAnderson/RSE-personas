"""Checks whether given GitHub repository is eligible for analysis."""

from pathlib import Path
import argparse
import datetime
import pandas as pd
import yaml
import logging

import utilities.get_default_logger as loggit


class GenerateStudySample:
    logger: logging.Logger
    in_notebook: bool
    current_date_info: str
    write_location: Path
    read_location: Path

    def __init__(
        self,
        in_notebook: bool,
        logger: None | logging.Logger = None,
    ) -> None:
        if logger is None:
            self.logger = loggit.get_default_logger(
                console=False,
                set_level_to="DEBUG",
                log_name="logs/pre-analysis_data_times_prep.txt",
                in_notebook=in_notebook,
            )
        else:
            self.logger = logger

        self.in_notebook = in_notebook
        # write-out file setup
        self.current_date_info = datetime.datetime.now().strftime(
            "%Y-%m-%d"
        )  # at start of script to avoid midnight/long-run issues
        self.read_location = Path("data/" if not in_notebook else "../../data/")
        self.write_location = Path("data/" if not in_notebook else "../../data/")

    def check_repo_eligibility(
        self,
        population_set: pd.DataFrame,
        read_location: str | Path = "data/",
        write_location: str | Path = "data/",
    ) -> pd.DataFrame | None:
        """
        Reads in summarised_repo_stats logfile generated by read_summary_stats_log.py
        and checks whether each GitHub repository meets eligibility criteria
        for analysis in coding-smart study.
        Returns csv file with additional colummn(s) of eligibility bool (?and additional information?)

        :param in_filename: name of CSV file to read in, excluding file extension. Default = "summarised_repo_stats".
        :type: str
        :param read_in_location: path of Github URLs file as string. Default = "data/"
        :type: str
        :returns: `summarised_repo_stats_eligible_df`
        :type: pd.DataFrame df equivalent to summarised_repo_stats with additional column(s)

        Examples:
        ----------
        >>> check_repo_eligibility(in_filename='summarised_repo_stats_2025-04-15.csv', read_in_location='data/')
        TODO
        """
        if population_set is None or population_set.empty:
            raise RuntimeError(
                "Population Set given is None or dataframe is empty; check input"
            )

        n_population = len(population_set.index)  # total population of RS repos

        # Read in summarised_repo_stats file generated by read_summary_stats_log.py

        # check each column (stat) in each row (repo) for inclusion/exclusion.

        # mark repo as included/excluded

        # generate NEW summarised_repo_stats file with subset of eligible repos?

        # This function will require processed df output
        # `summarised_repo_stats_2025-03-26.csv` from log file output
        # created by githubanalysis.processing.summarise_repo_stats() function.

        # Will return repo_eligible = True if repo meets inclusion/exclusion criteria.
        # Criteria should be set either in separate config file or as function arguments (ease of use/reproducibility)

        # Dev Numbers:
        # repo has > 1 dev
        # repo has < 1000 devs

        # Commits:
        # > 100 commits? > 500?

        # Has PRs:
        # has PRs

        # Is RS:
        # (will be TRUE since the gh repo url should have come via zenodocode therefore has associated DOI)

        # Issue Tickets:
        # uses issue tickets
        # has > 10 issue tickets

        # Recent Activity:
        # has commit within last 12 months? 18 months?
        # last PR activity within last 12 months?

        # Repo Age:
        # repo is established, >3 years old.

        # Licence:
        # has open license allowing me to work w/ it
        yamlpath = Path(read_location, "permissive_licenses.yml")
        self.logger.info(f"License yaml file at: {yamlpath}")

        try:
            with open(file=yamlpath, mode="r") as file:
                licenses = yaml.safe_load(file)
            self.logger.info(licenses)
        except:
            self.logger.info("licenses ain't right")
            raise RuntimeError("yaml file isn't loading somehow")

        # self.logger.info(f"{licenses['accept_licenses']}")

        # population_set.loc[:, "licensed_to_use"] = population_set["repo_license"].apply(
        #     lambda x: x in licenses["accept_licenses"]
        # )

        # Accessibility:
        # repo is set to public

        # ?? Repo Content:
        # repo contains code, not just docs or data.
        # look for file endings.

        # Repo Language:
        # contains some of: python, (shell?), (R?), java, C, C++, (FORTRAN???)

        # permissive licenses

        # # excluded set:
        # binned = population_set.query(
        #     "licensed_to_use == False or repo_is_fork == True or devs_above_mean == False or devs_within_2SD == False or repo_age_days < 1000"
        # )

        # excl_dataset_repos = len(binned)  # EXCLUSIONS

        # print(
        #     f"{excl_dataset_repos} to be excluded: no license, is fork, less than 10devs, more than 2SD of devs"
        # )

        # print(
        #     f"This means {n_population - excl_dataset_repos} ({round((n_population - excl_dataset_repos) / n_population*100, 2)}%) REMAIN of total {n_population} population RS repos"
        # )

        eligible_repos = population_set  # <<<<<< EDIT THIS AFTER IMPLEMENTING CRITERIA!

        return eligible_repos

    def read_population_stats(
        self,
        summary_repo_stats_file="summarised_repo_stats.csv",
        read_location: str | Path = "data/",
        write_location: str | Path = "data/",
    ):
        start_time = datetime.datetime.now()

        population_set = pd.read_csv(
            filepath_or_buffer=summary_repo_stats_file, header=0
        )
        self.logger.info(f"population_set dataframe has {len(population_set)} rows.")

        if population_set is None or population_set.empty:
            raise RuntimeError(
                "Population Set given is None or dataframe is empty; check input"
            )

        population_set.drop_duplicates(keep="last", inplace=True)

        self.logger.info(
            f"population_set df has {population_set.groupby('repo_name').ngroups} unique repo_names AFTER DEDUPLICATION (all columns)."
        )

        population_set.drop_duplicates(subset=["repo_name"], keep="last", inplace=True)

        self.logger.info(
            f"population_set df has {population_set.groupby('repo_name').ngroups} unique repo_names AFTER DEDUPLICATION (repo_name only)."
        )

        self.logger.info(
            f"Population dataset contains {population_set.groupby('repo_name').ngroups} repositories"
        )

        end_time = datetime.datetime.now()

        logger.info(
            f"Run time for {population_set.groupby('repo_name').ngroups} repos: {end_time - start_time}."
        )
        return population_set


parser = argparse.ArgumentParser()
parser.add_argument(
    "-f",
    "--summary-repo-data-file",
    metavar="SUMMARY_REPO_DATA_FILE",
    help="Path to .csv file containing commits data (line per repo-individual), eg 'commits-data-per-dev_x5828-repos_2025-02-15.csv'.",
    type=str,
)


if __name__ == "__main__":
    args = parser.parse_args()
    summary_repo_file: str = args.summary_repo_data_file

    logger = loggit.get_default_logger(
        console=True,
        set_level_to="DEBUG",
        log_name="logs/pre-analysis_data_times_preps_logs.txt",
        in_notebook=False,
    )

    logger.info("Running eligibility check process.")

    generate_study_sample = GenerateStudySample(in_notebook=False, logger=logger)

    population_set = generate_study_sample.read_population_stats(
        summary_repo_stats_file=summary_repo_file,
        read_location="data/",
        write_location="data/",
    )
    try:
        if population_set is not None:
            # apply the eligibility criteria function to the population dataset
            study_sample = generate_study_sample.check_repo_eligibility(
                population_set=population_set
            )
            if study_sample is not None:
                logger.info(
                    f"There are {study_sample.groupby('repo_name').ngroups} eligible repositories from a total of {population_set.groupby('repo_name').ngroups} population repositories."
                )
                logger.info(f"Study sample df has length: {len(study_sample)}.")

                sample_pc = round(
                    (  # generate rounded (2dp) percentage: sample of population
                        (
                            study_sample.groupby("repo_name").ngroups
                            / population_set.groupby("repo_name").ngroups
                        )
                        * 100
                    ),
                    2,
                )
                logger.info(
                    f"Study Sample ({study_sample.groupby('repo_name').ngroups} repos) represents {sample_pc}% of Population Set ({population_set.groupby('repo_name').ngroups} repos)."
                )

    except Exception as e:
        logger.error(
            f"Error in generating the study sample from the population set repo summary data; {e}; error type: {type(e)}"
        )
